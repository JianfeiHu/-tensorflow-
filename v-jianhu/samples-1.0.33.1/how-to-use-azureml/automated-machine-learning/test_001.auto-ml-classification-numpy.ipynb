{
    "cells":  [
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "# AutoML 001: Classification numpy\n",
                                     "\n",
                                     "In this example we use the scikit learn\u0027s [digit dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) to showcase how you can use the AutoML Classifier for a simple classification problem.\n",
                                     "\n",
                                     "Make sure you have executed the [setup](setup.ipynb) before running this notebook.\n",
                                     "\n",
                                     "In this notebook you would see\n",
                                     "1. Creating or reusing an existing Project and Workspace\n",
                                     "2. Instantiating AutoML Classifier\n",
                                     "3. Training the Model using local compute\n",
                                     "4. Exploring the results\n",
                                     "5. Testing the fitted model\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Create Project and Workspace\n",
                                     "\n",
                                     "As part of the setup you have already created a workspace. For AutoML you would need to create a \u003cb\u003eProject\u003c/b\u003e. A Project is a local folder that contains files for your Azure ML experiments. It is associated with a run history, a cloud container of run metrics and output artifacts from your experiments. You can either attach a local folder as a new project, or load a local folder as a project if it has been attached before."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import logging\n",
                                     "\n",
                                     "from matplotlib import pyplot as plt\n",
                                     "import numpy as np\n",
                                     "import pandas as pd\n",
                                     "from sklearn import datasets\n",
                                     "\n",
                                     "import azureml.core\n",
                                     "from azureml.core.experiment import Experiment\n",
                                     "from azureml.core.workspace import Workspace\n",
                                     "from azureml.train.automl import AutoMLConfig"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "ws = Workspace.from_config()\n",
                                     "\n",
                                     "# choose a name for the run history container in the workspace\n",
                                     "experiment_name = \u0027automl-classification-numpy\u0027\n",
                                     "# project folder\n",
                                     "project_folder = \u0027./sample_projects/automl-classification-numpy\u0027\n",
                                     "\n",
                                     "experiment = Experiment(ws, experiment_name)\n",
                                     "\n",
                                     "output = {}\n",
                                     "output[\u0027SDK version\u0027] = azureml.core.VERSION\n",
                                     "output[\u0027Subscription ID\u0027] = ws.subscription_id\n",
                                     "output[\u0027Workspace\u0027] = ws.name\n",
                                     "output[\u0027Resource Group\u0027] = ws.resource_group\n",
                                     "output[\u0027Location\u0027] = ws.location\n",
                                     "output[\u0027Project Directory\u0027] = project_folder\n",
                                     "output[\u0027Run History Name\u0027] = experiment_name\n",
                                     "pd.set_option(\u0027display.max_colwidth\u0027, -1)\n",
                                     "pd.DataFrame(data=output, index=[\u0027\u0027]).T"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Diagnostics\n",
                                     "Opt-in diagnostics collection for better experience, quality, and security of future releases"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "from azureml.telemetry import set_diagnostics_collection\n",
                                     "set_diagnostics_collection(send_diagnostics=True)"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "Set your primary metric:"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {
                                       "template_template_cell":  true
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "primary_metric = \"AUC_weighted\"\n",
                                     "data_library = \"numpy\""
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "# Load Digits Dataset"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "digits = datasets.load_digits()\n",
                                     "\n",
                                     "# only take the first 100 rows if you want the training steps to run faster\n",
                                     "#X_digits = digits.data[100:,:]\n",
                                     "#y_digits = digits.target[100:]\n",
                                     "\n",
                                     "# use full dataset\n",
                                     "X_digits = digits.data\n",
                                     "y_digits = digits.target"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Instantiate Auto ML Config\n",
                                     "\n",
                                     "Instantiate a AutoMLConfig object. This defines the settings and data used to run the experiment.\n",
                                     "\n",
                                     "|Property|Description|\n",
                                     "|-|-|\n",
                                     "|**task**|classification or regression|\n",
                                     "|**primary_metric**|This is the metric that you want to optimize.\u003cbr\u003e Classification supports the following primary metrics \u003cbr\u003e\u003ci\u003eaccuracy\u003c/i\u003e\u003cbr\u003e\u003ci\u003eAUC_weighted\u003c/i\u003e\u003cbr\u003e\u003ci\u003ebalanced_accuracy\u003c/i\u003e\u003cbr\u003e\u003ci\u003eaverage_precision_score_weighted\u003c/i\u003e\u003cbr\u003e\u003ci\u003eprecision_score_weighted\u003c/i\u003e|\n",
                                     "|**iteration_timeout_minutes**|Time limit in minutes for each iterations|\n",
                                     "|**iterations**|Number of iterations. In each iteration Auto ML trains the data with a specific pipeline|\n",
                                     "|**n_cross_validations**|Number of cross validation splits|\n",
                                     "|**X**|(sparse) array-like, shape = [n_samples, n_features]|\n",
                                     "|**y**|(sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]\u003cbr\u003eMulti-class targets. An indicator matrix turns on multilabel classification.  This should be an array of integers. |\n",
                                     "|**path**|Relative path to the project folder.  AutoML stores configuration files for the experiment under this folder. You can specify a new empty folder. |"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "automl_config = None\n",
                                     "\n",
                                     "def InitAutoMLConfig():\n",
                                     "    global automl_config    \n",
                                     "    X_data = X_digits\n",
                                     "    y_data = y_digits\n",
                                     "    \n",
                                     "    if (data_library == \u0027pandas\u0027):\n",
                                     "        X_data = pd.DataFrame(X_digits) #intentionally y data is skipped since pandas returns it as 2-d array and we need 1-d array       \n",
                                     "        \n",
                                     "    automl_config = AutoMLConfig(task = \u0027classification\u0027,\n",
                                     "                                     debug_log=\"{0}_{1}_normal.log\".format(primary_metric, data_library),\n",
                                     "                                     primary_metric = primary_metric,\n",
                                     "                                     iteration_timeout_minutes = 60,\n",
                                     "                                     iterations = 10,\n",
                                     "                                     X = X_data, \n",
                                     "                                     y = y_data,\n",
                                     "                                     n_cross_validations = 2,\n",
                                     "                                     verbosity=logging.INFO\n",
                                     "                                    )    \n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "local_run = None\n",
                                     "\n",
                                     "def Submit():    \n",
                                     "    global local_run\n",
                                     "    local_run = experiment.submit(automl_config, show_output = True)"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Exploring the results"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Retrieve the Best Model\n",
                                     "\n",
                                     "Below we select the best pipeline from our iterations. The *get_output* method on automl_classifier returns the best run and the fitted model for the last *fit* invocation. There are overloads on *get_output* that allow you to retrieve the best run and fitted model for *any* logged metric or a particular *iteration*."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "def ValidateBestFitPrimaryMetric():\n",
                                     "    best_run, fitted_model = local_run.get_output()\n",
                                     "    metric_value = best_run.get_metrics()[primary_metric]\n",
                                     "    lower_limit = .93\n",
                                     "    if primary_metric == \u0027norm_macro_recall\u0027:\n",
                                     "        lower_limit = .7\n",
                                     "        \n",
                                     "    if not (lower_limit \u003c float(metric_value) \u003c= 1):\n",
                                     "        raise Exception(\u0027Metric value of {0} is not in the valid range.\u0027.format(metric_value))\n",
                                     "    print(\"\\n Finished running \u0027ValidateBestFitPrimaryMetric\u0027\")    "
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "#### Best Model based on any other metric"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "def ValidateBestFitOtherMetric():\n",
                                     "    best_run, fitted_model = local_run.get_output(metric=primary_metric)\n",
                                     "    if fitted_model == None:\n",
                                     "        raise Exception(\u0027Fitted model is None for {metric}.\u0027.format(metric=primary_metric))\n",
                                     "    print(\"\\n Finished running \u0027ValidateBestFitOtherMetric\u0027\")    "
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "#### Best Model based on any iteration"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "def ValidateAllModelsPrimaryMetric():\n",
                                     "    for iteration in range(0, 10):\n",
                                     "        best_run, fitted_model = local_run.get_output(iteration=iteration)        \n",
                                     "        try:\n",
                                     "            fitted_model.predict(X_digits[[0]])\n",
                                     "        except Exception as e:\n",
                                     "            raise Exception(\u0027Invalid fitted model returned for iteration\u0027\n",
                                     "                            \u0027 {0} for AUC_macro.\u0027.format(iteration)) from e\n",
                                     "    print(\"\\n Finished running \u0027ValidateAllModelsPrimaryMetric\u0027\")     "
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Testing our best pipeline"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "def TestPipeline():\n",
                                     "    #load test data\n",
                                     "    digits = datasets.load_digits()\n",
                                     "    X_digits = digits.data[:10, :]\n",
                                     "    y_digits = digits.target[:10]\n",
                                     "    images = digits.images[:10]\n",
                                     "\n",
                                     "    #Randomly select digits and test\n",
                                     "    best_run, fitted_model = local_run.get_output()\n",
                                     "    for index in np.random.choice(len(y_digits), 2):\n",
                                     "        print(index)\n",
                                     "        predicted = fitted_model.predict(pd.DataFrame(X_digits[index:index + 1]) if data_library == \"pandas\" else X_digits[index:index + 1])[0]\n",
                                     "        label = y_digits[index]\n",
                                     "        title = \"Label value = %d  Predicted value = %d \" % ( label,predicted)\n",
                                     "        fig = plt.figure(1, figsize=(3,3))\n",
                                     "        ax1 = fig.add_axes((0,0,.8,.8))\n",
                                     "        ax1.set_title(title)\n",
                                     "        plt.imshow(images[index], cmap=plt.cm.gray_r, interpolation=\u0027nearest\u0027)\n",
                                     "        plt.show()"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "### Test other primary metrics and data libraries\n",
                                     "\n",
                                     "We can do the same steps for other metrics."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "steps = [ InitAutoMLConfig, Submit, ValidateBestFitPrimaryMetric, ValidateBestFitOtherMetric, ValidateAllModelsPrimaryMetric, TestPipeline]\n",
                                     "primary_metrics = [\u0027accuracy\u0027, \u0027precision_score_weighted\u0027, \u0027norm_macro_recall\u0027]\n",
                                     "print(\"data_library is \u0027%s\u0027\" % (data_library))\n",
                                     "for metric in primary_metrics:    \n",
                                     "    primary_metric = metric\n",
                                     "    print(\"primary_metric is \u0027%s\u0027\" % (primary_metric))\n",
                                     "    for step in steps:\n",
                                     "        step()"
                                 ]
                  }
              ],
    "metadata":  {
                     "authors":  [
                                     {
                                         "name":  "savitam"
                                     }
                                 ],
                     "kernelspec":  {
                                        "display_name":  "Python 3.6 - AzureML",
                                        "language":  "python",
                                        "name":  "python3-azureml"
                                    },
                     "language_info":  {
                                           "codemirror_mode":  {
                                                                   "name":  "ipython",
                                                                   "version":  3
                                                               },
                                           "file_extension":  ".py",
                                           "mimetype":  "text/x-python",
                                           "name":  "python",
                                           "nbconvert_exporter":  "python",
                                           "pygments_lexer":  "ipython3",
                                           "version":  "3.6.5"
                                       }
                 },
    "nbformat":  4,
    "nbformat_minor":  2
}
